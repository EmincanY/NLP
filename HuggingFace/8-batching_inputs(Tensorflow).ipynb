{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1045, 1005, 2310, 2042, 3403, 2005, 1037, 7861, 2912, 2914, 2607, 2026, 2878, 2166, 1012]\n",
      "[1045, 2293, 2023, 1012]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "sentences = [\n",
    "    \"I've been waiting for a EmKa Academy course my whole life.\",\n",
    "    \"I love this.\"\n",
    "]\n",
    "tokens = [tokenizer.tokenize(sentence) for sentence in sentences]\n",
    "ids = [tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
    "print(ids[0])\n",
    "print(ids[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't convert non-rectangular Python sequence to Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Emincan\\Desktop\\huggingface\\batching_inputs(Tensorflow).ipynb Cell 2\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Emincan/Desktop/huggingface/batching_inputs%28Tensorflow%29.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Emincan/Desktop/huggingface/batching_inputs%28Tensorflow%29.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m ids \u001b[39m=\u001b[39m [\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Emincan/Desktop/huggingface/batching_inputs%28Tensorflow%29.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     [\u001b[39m1045\u001b[39m, \u001b[39m1005\u001b[39m, \u001b[39m2310\u001b[39m, \u001b[39m2042\u001b[39m, \u001b[39m3403\u001b[39m, \u001b[39m2005\u001b[39m, \u001b[39m1037\u001b[39m, \u001b[39m7861\u001b[39m, \u001b[39m2912\u001b[39m, \u001b[39m2914\u001b[39m, \u001b[39m2607\u001b[39m, \u001b[39m2878\u001b[39m, \u001b[39m2026\u001b[39m, \u001b[39m2166\u001b[39m, \u001b[39m1012\u001b[39m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Emincan/Desktop/huggingface/batching_inputs%28Tensorflow%29.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     [\u001b[39m1045\u001b[39m, \u001b[39m2293\u001b[39m, \u001b[39m2023\u001b[39m, \u001b[39m1012\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Emincan/Desktop/huggingface/batching_inputs%28Tensorflow%29.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m ]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Emincan/Desktop/huggingface/batching_inputs%28Tensorflow%29.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m input_ids \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mconstant(ids) \u001b[39m# Its automatically padded. So in tensorflow you can create tensor with different lengths.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Emincan\\miniconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:263\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[0;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    168\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \n\u001b[0;32m    170\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 263\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    264\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\Emincan\\miniconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:275\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    273\u001b[0m     \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39m\"\u001b[39m\u001b[39mtf.constant\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    274\u001b[0m       \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m--> 275\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m    277\u001b[0m const_tensor \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39m_create_graph_constant(  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    278\u001b[0m     value, dtype, shape, name, verify_shape, allow_broadcast\n\u001b[0;32m    279\u001b[0m )\n\u001b[0;32m    280\u001b[0m \u001b[39mreturn\u001b[39;00m const_tensor\n",
      "File \u001b[1;32mc:\\Users\\Emincan\\miniconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:285\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[0;32m    284\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 285\u001b[0m   t \u001b[39m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[0;32m    286\u001b[0m   \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    287\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n",
      "File \u001b[1;32mc:\\Users\\Emincan\\miniconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     96\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m     97\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Can't convert non-rectangular Python sequence to Tensor."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "ids = [\n",
    "    [1045, 1005, 2310, 2042, 3403, 2005, 1037, 7861, 2912, 2914, 2607, 2878, 2026, 2166, 1012],\n",
    "    [1045, 2293, 2023, 1012]\n",
    "]\n",
    "\n",
    "input_ids = tf.constant(ids) # This happens because of different lengths. They have to be in same length to be tensored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 15), dtype=int32, numpy=\n",
       "array([[1045, 1005, 2310, 2042, 3403, 2005, 1037, 7861, 2912, 2914, 2607,\n",
       "        2878, 2026, 2166, 1012],\n",
       "       [1045, 2293, 2023, 1012,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0]])>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = [\n",
    "    [1045, 1005, 2310, 2042, 3403, 2005, 1037, 7861, 2912, 2914, 2607, 2878, 2026, 2166, 1012],\n",
    "    [1045, 2293, 2023, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "]\n",
    "\n",
    "input_ids = tf.constant(ids)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 0.38173926 -0.2528047 ]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor([[-4.135063   4.4688683]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.3817402  -0.25280547]\n",
      " [-2.3545122   2.4671905 ]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "ids1 = tf.constant([[1045, 1005, 2310, 2042, 3403, 2005, 1037, 7861, 2912, 2914, 2607, 2878, 2026, 2166, 1012]])\n",
    "\n",
    "ids2 = tf.constant([[1045, 2293, 2023, 1012]])\n",
    "\n",
    "all_ids = tf.constant(\n",
    "    [\n",
    "    [1045, 1005, 2310, 2042, 3403, 2005, 1037, 7861, 2912, 2914, 2607, 2878, 2026, 2166, 1012],\n",
    "    [1045, 2293, 2023, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] # attention layers to ignore the padding tokens. Thats why we need to pass them an attention mask.\n",
    "]\n",
    ")\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "print(model(ids1).logits)\n",
    "print(model(ids2).logits)\n",
    "print(model(all_ids).logits) # ids2 logits changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = tf.constant(\n",
    "    [\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] # attention layers to ignore the padding tokens. Thats why we need to pass them an attention mask.\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 0.38173926 -0.2528047 ]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor([[-4.135063   4.4688683]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "output1 = model(ids1)\n",
    "output2 = model(ids2)\n",
    "print(output1.logits)\n",
    "print(output2.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.3817402  -0.25280547]\n",
      " [-4.135063    4.468869  ]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "output = model(all_ids, attention_mask = attention_mask)\n",
    "print(output.logits) # Same results now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
