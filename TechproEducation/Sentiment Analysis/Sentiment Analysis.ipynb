{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aaa44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Embedding,LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# from warnings import filterwarnings\n",
    "# filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb4912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0937a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "WPT = nltk.WordPunctTokenizer()\n",
    "stop_word_list = nltk.corpus.stopwords.words('english')\n",
    "stop_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3a464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a034b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"comments.xlsx\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbdae81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cleaning(data):\n",
    "#     #1. Tokenize\n",
    "#     text_tokens = word_tokenize(data.replace(\"'\", \"\").lower())\n",
    "#     #2. Remove Puncs and numbers\n",
    "#     tokens_without_punc = [w for w in text_tokens if w.isalpha()]\n",
    "#     #3. Removing Stopwords\n",
    "#     tokens_without_sw = [t for t in tokens_without_punc if t not in stop_words]\n",
    "#     #4. lemma\n",
    "#     text_cleaned = [WordNetLemmatizer().lemmatize(t) for t in tokens_without_sw]\n",
    "#     #joining\n",
    "#     return \" \".join(text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1590fba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the process of cleaning the punctuation marks found in our data\n",
    "df[\"comments\"] = df[\"comments\"].apply(lambda x: re.sub('[,\\.!?:()\"Ÿ˜Š]', '', str(x)))\n",
    "#conversion of uppercase letters to lowercase\n",
    "df[\"comments\"] = df[\"comments\"].apply(lambda x: x.lower())\n",
    "#cleaning of extra spaces\n",
    "df[\"comments\"] = df[\"comments\"].apply(lambda x: x.strip())\n",
    "#removal of stopwords contained in sentences\n",
    "def token(comment):\n",
    "    words = nltk.tokenize.word_tokenize(comment)\n",
    "    filtered_words = [word for word in words if word not in stop_word_list]\n",
    "    not_stopword_doc = \" \".join(filtered_words)\n",
    "    return not_stopword_doc\n",
    "df[\"comments\"] = df[\"comments\"].apply(lambda x: token(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef590c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"comments\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6e3d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['comments','rating']]\n",
    "df = df[df[\"comments\"] != \"nan\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9cdc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a363570",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80774aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f6a88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rating'] = df['rating'].map({5:'1' , 4:'1' , 1:'0' , 2:'0' , 3: np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8a3092",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rating'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0050183",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcc695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rating'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0611d4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"comments\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a92236",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145c9e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[\"comments\"].values.tolist()\n",
    "comments = df[\"comments\"].values.tolist()\n",
    "sentiments = df['rating'].values.tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(comments,sentiments,test_size = 0.15, random_state = 53 , stratify=sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0b1e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85c7aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"comments\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3d8134",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = \" \".join(df[\"comments\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a301a2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7123c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(word_tokenize(all_words)) # How many words(words) do we have, we're looking at this.\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5cf294",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cad05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = 10000)\n",
    "# tokenizer.fit_on_texts(df['comments'])\n",
    "tokenizer.fit_on_texts(comments)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa453c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokens = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_tokens = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2826bf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = [len(tokens) for tokens in X_train_tokens + X_test_tokens] # The number of individual tokens for each comment in the Train and Test comments\n",
    "num_tokens = np.array(num_tokens)\n",
    "num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b181ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum number of words that can be in a comment\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6cb639",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7053acbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(num_tokens < max_tokens) / len(num_tokens) # 95% of our comments are ideal comments that do not exceed the max word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770658bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#it is necessary to bring each comment to the same size, this is how RNN works.\n",
    "X_train_pad = pad_sequences(X_train_tokens, maxlen=max_tokens)\n",
    "X_test_pad = pad_sequences(X_test_tokens, maxlen=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdd5f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6caac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function needs to be written in order for tokenized words to become strings again.\n",
    "\n",
    "# idx = tokenizer.word_index\n",
    "# inverse_map = dict(zip(idx.values(), idx.keys()))\n",
    "# #tokenlaştırılan cümleyi tekrar string hale getirmek\n",
    "# def tokens_to_string(tokens):\n",
    "#     words = [inverse_map[token] for token in tokens if token !=0]\n",
    "#     text = ' '.join(words)\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2be515",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# A vector of length 50 is created corresponding to each word. (Embedding matrix)\n",
    "embedding_size = 50\n",
    "# The matrix will be the number of words and the size of the embedding, that is, it will be 10 by 50 long. This is also given a name with the name variable.\n",
    "model.add(Embedding(input_dim=10000,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=max_tokens,\n",
    "                    name='embedding_layer'))\n",
    "## LSTM with 16 neurons (with 16 outputs, return_sequences=True means give the entire output)\n",
    "model.add(LSTM(units=16, return_sequences=True))\n",
    "## LSTM with 8 neurons (with 8 outputs, return_sequences=True means give the entire output)\n",
    "model.add(LSTM(units=8, return_sequences=True))\n",
    "# LSTM with 4 neurons (with 4 outputs, return_sequences=False, i.e. the default value, will give a single output)\n",
    "model.add(LSTM(units=4))\n",
    "## The output layer is used when displaying the dense layer visually. Since it consists of a single neuron, it is written 1.\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "# optimizer = Adam(lr=0.001)\n",
    "\n",
    "# To compile the model, the loss function binary_crossentropy -> is used only for 2 classes, but categorical_crossentropy for more classes.\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              #optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38f84eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed38b55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation can be performed according to the situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814162c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list()\n",
    "for x in y_train:\n",
    "    a.append(np.fromstring(x, dtype=np.int, sep=','))\n",
    "y_train = np.array(a)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e458fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636d66d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training, going through the training once -> epoch, batch_size -> 16\n",
    "history = model.fit(X_train_pad, np.array(y_train), epochs=50, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f6cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056722a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss = pd.DataFrame(model.history.history)\n",
    "model_loss.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0709a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "7a1a0460069fc11b3fb92be4d9fd1cc415d13eb6d6391afc101a1435d589d745"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
